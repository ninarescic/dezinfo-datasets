{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43c7e250",
   "metadata": {},
   "source": [
    "# TGDataset (Zenodo 7640712) — Mounted Archive Overview\n",
    "\n",
    "This notebook inspects **TGDataset** when it is mounted on your server as one or more `*.tar.gz` parts (e.g. `TGDataset_1.tar.gz`, ...). It **does not download or extract** anything; it reads metadata and small samples directly from the archives."
   ]
  },
  {
   "cell_type": "code",
   "id": "7ce2c559",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T16:29:47.588452Z",
     "start_time": "2026-01-30T13:52:44.507152Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# 0) Project setup (same pattern as your other notebooks)\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"src\").is_dir():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find repo root containing `src/`\")\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from src.core.config import SETTINGS  # noqa: E402\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"DATA_ROOT:\", SETTINGS.DATA_ROOT)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: /mnt/c/Users/rescic/PycharmProjects/dezinfo-datasets\n",
      "DATA_ROOT: /home/rescic/dezinfo_data\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "8b823844",
   "metadata": {},
   "source": [
    "## Locate the mounted TGDataset archives\n",
    "\n",
    "We assume your mount folder is named `tgdataset` under `SETTINGS.DATA_ROOT`, and it contains one or more `*.tar.gz` parts (e.g. `TGDataset_1.tar.gz` ...). We collect all archives and sort them for stable processing."
   ]
  },
  {
   "cell_type": "code",
   "id": "ff9caf8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T16:30:37.567238Z",
     "start_time": "2026-01-30T16:30:37.553403Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# 1) Mounted dataset path (NO downloading / NO extracting)\n",
    "# ============================================================\n",
    "DATASET_SUBDIR = \"tgdataset\"   # <-- mount folder name (you said this matches)\n",
    "DATASET_DIR = Path(SETTINGS.DATA_ROOT) / DATASET_SUBDIR\n",
    "assert DATASET_DIR.exists(), f\"Mounted dataset folder not found: {DATASET_DIR}\"\n",
    "print(\"DATASET_DIR:\", DATASET_DIR)\n",
    "\n",
    "# Find *all* tar.gz parts (e.g., TGDataset_1.tar.gz ... TGDataset_4.tar.gz)\n",
    "TAR_PATHS = sorted(DATASET_DIR.glob(\"*.tar.gz\"))\n",
    "assert TAR_PATHS, f\"No .tar.gz files found in {DATASET_DIR}\"\n",
    "\n",
    "print(\"\\nArchives found:\")\n",
    "for p in TAR_PATHS:\n",
    "    print(\" -\", p.name)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR: /home/rescic/dezinfo_data/tgdataset\n",
      "\n",
      "Archives found:\n",
      " - TGDataset_1.tar.gz\n",
      " - TGDataset_2.tar.gz\n",
      " - TGDataset_3.tar.gz\n",
      " - TGDataset_4.tar.gz\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "5a728b8f",
   "metadata": {},
   "source": [
    "## List archive contents (quick peek)\n",
    "\n",
    "We print a short listing of the first few file members in each archive so you can see what types of files are stored inside (JSON/JSONL, CSV, etc.). This step reads **only metadata**, not the full file contents."
   ]
  },
  {
   "cell_type": "code",
   "id": "628569ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T16:29:47.654868Z",
     "start_time": "2026-01-30T13:52:44.875405Z"
    }
   },
   "source": [
    "import tarfile\n",
    "\n",
    "def list_tar_members(tar_path: Path, max_members: int = 30) -> list[tuple[str, int]]:\n",
    "    with tarfile.open(tar_path, mode=\"r:gz\") as tf:\n",
    "        files = [m for m in tf.getmembers() if m.isfile()]\n",
    "        return [(m.name, m.size) for m in files[:max_members]]\n",
    "\n",
    "for tp in TAR_PATHS:\n",
    "    print(f\"\\n== {tp.name} ==\")\n",
    "    for name, size in list_tar_members(tp, max_members=15):\n",
    "        print(f\"{size:12d}  {name}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "== TGDataset_1.tar.gz ==\n",
      "  4729515989  public_db/folder_0/file_28_from_ebaladf_to_elanvirtualgallery.json\n",
      "  5063177340  public_db/folder_0/file_32_from_FantastArt_to_fenixatelier.json\n",
      "  3543132657  public_db/folder_0/file_13_from_BlessKingss_to_Boss_proof.json\n",
      "  3877814451  public_db/folder_0/file_12_from_Bibliotecacir_to_blessher.json\n",
      "  3537316326  public_db/folder_0/file_22_from_DailyBriefing_to_daytejaby.json\n",
      "  4366933161  public_db/folder_0/file_17_from_channelforsell6_to_chrir.json\n",
      "  2788424285  public_db/folder_0/file_19_from_cnnethiopia_to_coronaRSAstatus.json\n",
      "  3584765176  public_db/folder_0/file_34_from_flowers_in_your_lungs_to_freeJuliane.json\n",
      "  3782020535  public_db/folder_0/file_27_from_drone_life_to_ebaka1337.json\n",
      "  3402382031  public_db/folder_0/file_33_from_fenixconservadora_to_flowersunicorn_shop.json\n",
      "  3618193723  public_db/folder_0/file_18_from_chrisaresoffiziell_to_CNNBrk.json\n",
      "  3946563459  public_db/folder_0/file_1_from_adhd_recharge_to_ahvvn.json\n",
      "  3530975727  public_db/folder_0/file_29_from_elaramonis_to_EPClassic.json\n",
      "  4059069819  public_db/folder_0/file_20_from_coronasatanismus_to_cryptoast_fr.json\n",
      "  4279604646  public_db/folder_0/file_26_from_dokhtar_mashhad_to_Drones_Belarus.json\n",
      "\n",
      "== TGDataset_2.tar.gz ==\n",
      "  7394076472  public_db/folder_1/file_66_from_mojnews_com_to_moveistv.json\n",
      "  3732339668  public_db/folder_1/file_41_from_harkatensani_org_to_hfrissara.json\n",
      "  3847460568  public_db/folder_1/file_60_from_MagazinesArchive_to_MaQaweli.json\n",
      "  3390730776  public_db/folder_1/file_53_from_kinderkleidunghandmade_to_komivoytyr.json\n",
      "  2976806553  public_db/folder_1/file_59_from_LotteryBotChannel_to_MAGAZINES123.json\n",
      "  4631136342  public_db/folder_1/file_69_from_Nachbarn_STEHEN_AUF_to_NaturkindBlog.json\n",
      "  3248100193  public_db/folder_1/file_49_from_jarchy0273_to_JSlayUSofA.json\n",
      "  4422693850  public_db/folder_1/file_62_from_maulipolice_to_Mehr_Infos_Hier.json\n",
      "  3867614446  public_db/folder_1/file_67_from_movementhistory_to_musaffo_uzb.json\n",
      "  3150249521  public_db/folder_1/file_57_from_lenakarnauhova_moda_to_limitless_center.json\n",
      "  4595059613  public_db/folder_1/file_61_from_maqivossanla_to_maukeb.json\n",
      "  4356721286  public_db/folder_1/file_55_from_krololo_girls_to_laguaridadezano.json\n",
      "  7279903544  public_db/folder_1/file_47_from_iraneayandeh_mv_to_ISwearToGodImAtheist.json\n",
      "  2523306629  public_db/folder_1/file_63_from_Mehr_o_baran_to_metro_architecture.json\n",
      "  4151879055  public_db/folder_1/file_54_from_komixpron_to_Krololophoto.json\n",
      "\n",
      "== TGDataset_3.tar.gz ==\n",
      "  4112369994  public_db/folder_2/file_74_from_NumbAlot_to_ohfuture.json\n",
      "  5053068295  public_db/folder_2/file_93_from_secret_history_TG_to_sgmeme.json\n",
      "  3792390399  public_db/folder_2/file_80_from_pismaturok_to_politdrug.json\n",
      "  4758545158  public_db/folder_2/file_81_from_politduhovka_to_po_reshal.json\n",
      "  3669883669  public_db/folder_2/file_72_from_NHKCCC_to_noor_muayad.json\n",
      "  3971422666  public_db/folder_2/file_102_from_Tanqidchi_to_techno_channel.json\n",
      "  3875412571  public_db/folder_2/file_96_from_sinformative2s_to_smotri_media.json\n",
      "  3778639680  public_db/folder_2/file_78_from_partyofthedead_to_PerplexedPen.json\n",
      "  4633113230  public_db/folder_2/file_70_from_NaturmedizinLICHTMensch_to_nespytsia.json\n",
      "  3298693381  public_db/folder_2/file_83_from_ProjectEpsilon_to_publichealthrussia.json\n",
      "  4175684956  public_db/folder_2/file_85_from_QtHoneypotGraphics_to_Rainbow1214.json\n",
      "  4750624552  public_db/folder_2/file_101_from_svinger_to_tanoneveshtpub.json\n",
      "  3706219155  public_db/folder_2/file_95_from_shitemislibratan_to_sinfln.json\n",
      "  3509115485  public_db/folder_2/file_103_from_techno_district_to_thankyoubodyrally.json\n",
      "  5570254557  public_db/folder_2/file_94_from_sgmemenewfoundland_to_shitdirtymarxistglobalistssay.json\n",
      "\n",
      "== TGDataset_4.tar.gz ==\n",
      "  5479643953  public_db/folder_3/file_113_from_vivela_vida_to_vsramem.json\n",
      "  4038181062  public_db/folder_3/file_116_from_wolfdogballsinc_to_XPOHOC_BPEMEHi.json\n",
      "  2745116403  public_db/folder_3/file_105_from_thesneaker_kingz_to_tinder.json\n",
      "  3306417207  public_db/folder_3/file_107_from_totheleft_to_TRUMPWON45.json\n",
      "  3923483048  public_db/folder_3/file_112_from_VENX9_to_vivelazik.json\n",
      "  3768146217  public_db/folder_3/file_114_from_vsratayatyan_to_weisstduschon.json\n",
      "  2293311970  public_db/folder_3/file_115_from_weit_blick_to_WOLFDAKSH.json\n",
      "  4282841340  public_db/folder_3/file_120_from_ZiyovuddinRahim_to___without_username__.json\n",
      "  3836707647  public_db/folder_3/file_118_from_ykt_novosty_to_zakssource.json\n",
      "  4235225368  public_db/folder_3/file_108_from_Trumpworld_to_t_21Wire.json\n",
      "  4859542612  public_db/folder_3/file_111_from_uzbunyodkor_to_venuuus27.json\n",
      "  5041011099  public_db/folder_3/file_119_from_zakuliska_to_Ziyouzkanali.json\n",
      "  4440074167  public_db/folder_3/file_110_from_United_Patriots_to_uzbtextile_com.json\n",
      "  4225892027  public_db/folder_3/file_106_from_TindeRandao_to_totgandekboldim.json\n",
      "  4139934633  public_db/folder_3/file_109_from_t_action_to_United_Manchester_Yunayted_Bruno.json\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "727ad352",
   "metadata": {},
   "source": [
    "## JSON footprint analysis inside compressed archives\n",
    "\n",
    "This section estimates how much JSON-like data exists in TGDataset.\n",
    "\n",
    "Key points:\n",
    "- We scan **all tar archives** and look for files ending in\n",
    "  `.json`, `.jsonl`, `.ndjson` (including `.gz` variants)\n",
    "- File sizes are taken from tar metadata (fast, no extraction)\n",
    "- We aggregate sizes by **top-level folder inside the archive**\n",
    "\n",
    "This gives a high-level view of where most of the structured data lives\n",
    "and how large it is, without loading any actual records."
   ]
  },
  {
   "cell_type": "code",
   "id": "b5520cf6",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "def sizeof_fmt(num: int, suffix=\"B\") -> str:\n",
    "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\"]:\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f} {unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    return f\"{num:.1f} Ei{suffix}\"\n",
    "\n",
    "json_like_suffixes = (\".json\", \".jsonl\", \".ndjson\", \".json.gz\", \".jsonl.gz\", \".ndjson.gz\")\n",
    "\n",
    "# We'll store lightweight member refs as dicts so we don't keep TarInfo objects around.\n",
    "json_members = []  # each: {\"tar\": Path, \"name\": str, \"size\": int}\n",
    "\n",
    "folder_bytes = defaultdict(int)\n",
    "total_bytes = 0\n",
    "\n",
    "for tar_path in TAR_PATHS:\n",
    "    with tarfile.open(tar_path, mode=\"r:gz\") as tf:\n",
    "        for m in tf.getmembers():\n",
    "            if not m.isfile():\n",
    "                continue\n",
    "            n = m.name\n",
    "            ln = n.lower()\n",
    "            if ln.endswith(json_like_suffixes):\n",
    "                json_members.append({\"tar\": tar_path, \"name\": n, \"size\": m.size})\n",
    "                total_bytes += m.size\n",
    "\n",
    "                parts = Path(n).parts\n",
    "                top = parts[0] if parts else \".\"\n",
    "                folder_bytes[top] += m.size\n",
    "\n",
    "print(\"JSON-like members found (across all tars):\", len(json_members))\n",
    "assert json_members, \"No JSON-like files were found inside the .tar.gz parts. Inspect archive contents above.\"\n",
    "\n",
    "print(\"Total JSON footprint (inside tars):\", sizeof_fmt(total_bytes))\n",
    "\n",
    "print(\"\\nTop-level folder breakdown (inside tars):\")\n",
    "for k, v in sorted(folder_bytes.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    print(f\"  - {k:25s} {sizeof_fmt(v)}\")\n",
    "\n",
    "print(\"\\nExample members (inside tars):\")\n",
    "for m in sorted(json_members, key=lambda d: (d[\"tar\"].name, d[\"name\"]))[:12]:\n",
    "    print(f\"  {m['tar'].name} :: {m['name']}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b66d30f9",
   "metadata": {},
   "source": [
    "## Peek at raw bytes to understand the JSON format\n",
    "\n",
    "If later sampling shows '0 channels', it usually means the files are **not line-delimited JSON**. Here we peek the first bytes of a representative member to detect whether it looks like:\n",
    "- JSON array (`[`)\n",
    "- JSON object (`{`)\n",
    "- JSONL/NDJSON (many lines each parseable)\n",
    "- gzip stream inside tar (magic bytes `1f 8b`)"
   ]
  },
  {
   "cell_type": "code",
   "id": "9c376c45",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T23:43:46.271718Z",
     "start_time": "2026-01-30T16:54:06.548407Z"
    }
   },
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "# 1) Point to your mounted folder\n",
    "DATASET_DIR = Path.home() / \"dezinfo_data\" / \"tgdataset\"  # <-- adjust if needed\n",
    "assert DATASET_DIR.exists(), f\"Mounted dataset folder not found: {DATASET_DIR}\"\n",
    "print(\"DATASET_DIR:\", DATASET_DIR)\n",
    "\n",
    "# 2) Discover tarballs (TGDataset_*.tar.gz)\n",
    "TAR_PATHS = sorted(DATASET_DIR.glob(\"*.tar.gz\"))\n",
    "assert TAR_PATHS, f\"No .tar.gz files found under {DATASET_DIR}\"\n",
    "print(\"Found tarballs:\")\n",
    "for p in TAR_PATHS:\n",
    "    print(\"  -\", p.name)\n",
    "\n",
    "# ---- sniff helper (yours, slightly made robust) ----\n",
    "def sniff_member_start(tar_path: Path, member_name: str, max_bytes: int = 2048) -> dict:\n",
    "    with tarfile.open(tar_path, mode=\"r:*\") as tf:  # r:* is more robust than r:gz\n",
    "        m = tf.getmember(member_name)\n",
    "        f = tf.extractfile(m)\n",
    "        assert f is not None, f\"Could not extract member: {member_name}\"\n",
    "        raw = f.read(max_bytes)\n",
    "\n",
    "    is_gzip_stream = raw[:2] == b\"\\x1f\\x8b\"\n",
    "\n",
    "    ws = b\" \\t\\r\\n\"\n",
    "    i = 0\n",
    "    while i < len(raw) and raw[i:i+1] in ws:\n",
    "        i += 1\n",
    "    first = raw[i:i+1] if i < len(raw) else b\"\"\n",
    "\n",
    "    return {\n",
    "        \"tar\": tar_path.name,\n",
    "        \"member\": member_name,\n",
    "        \"first_non_ws_byte\": first,\n",
    "        \"first_non_ws_char\": first.decode(\"utf-8\", errors=\"replace\") if first else \"\",\n",
    "        \"looks_like_array\": first == b\"[\",\n",
    "        \"looks_like_object\": first == b\"{\",\n",
    "        \"nested_gzip_stream\": is_gzip_stream,\n",
    "    }\n",
    "\n",
    "# 3) Collect members across tarballs\n",
    "def collect_members(tar_path: Path):\n",
    "    out = []\n",
    "    with tarfile.open(tar_path, mode=\"r:*\") as tf:\n",
    "        for m in tf.getmembers():\n",
    "            if m.isfile():\n",
    "                out.append({\"tar\": tar_path, \"name\": m.name})\n",
    "    return out\n",
    "\n",
    "members = []\n",
    "for tp in TAR_PATHS:\n",
    "    members.extend(collect_members(tp))\n",
    "\n",
    "print(f\"Collected {len(members):,} file members from {len(TAR_PATHS)} tarballs.\")\n",
    "\n",
    "# 4) Pick “JSON-ish” candidates by filename; fallback to everything\n",
    "json_members = [\n",
    "    d for d in members\n",
    "    if d[\"name\"].lower().endswith((\".json\", \".jsonl\"))\n",
    "       or \".jsonl\" in d[\"name\"].lower()\n",
    "       or \".json\" in d[\"name\"].lower()\n",
    "]\n",
    "print(f\"JSON-ish members: {len(json_members):,}\")\n",
    "\n",
    "candidates = json_members if json_members else members\n",
    "\n",
    "# 5) Sniff one deterministic sample\n",
    "sample_ref = sorted(candidates, key=lambda d: (d[\"tar\"].name, d[\"name\"]))[0]\n",
    "sniff = sniff_member_start(sample_ref[\"tar\"], sample_ref[\"name\"]) .\n",
    "sniff\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR: /home/rescic/dezinfo_data/tgdataset\n",
      "Found tarballs:\n",
      "  - TGDataset_1.tar.gz\n",
      "  - TGDataset_2.tar.gz\n",
      "  - TGDataset_3.tar.gz\n",
      "  - TGDataset_4.tar.gz\n"
     ]
    },
    {
     "ename": "ConnectionAbortedError",
     "evalue": "[Errno 103] Software caused connection abort",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOSError\u001B[39m                                   Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 46\u001B[39m, in \u001B[36mcollect_members\u001B[39m\u001B[34m(tar_path)\u001B[39m\n\u001B[32m     45\u001B[39m \u001B[38;5;28;01mwith\u001B[39;00m tarfile.open(tar_path, mode=\u001B[33m\"\u001B[39m\u001B[33mr:*\u001B[39m\u001B[33m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m tf:\n\u001B[32m---> \u001B[39m\u001B[32m46\u001B[39m     \u001B[38;5;28;01mfor\u001B[39;00m m \u001B[38;5;129;01min\u001B[39;00m \u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetmembers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m:\n\u001B[32m     47\u001B[39m         \u001B[38;5;28;01mif\u001B[39;00m m.isfile():\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/tarfile.py:2047\u001B[39m, in \u001B[36mTarFile.getmembers\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2046\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._loaded:    \u001B[38;5;66;03m# if we want to obtain a list of\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2047\u001B[39m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_load\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m        \u001B[38;5;66;03m# all members, we first have to\u001B[39;00m\n\u001B[32m   2048\u001B[39m                         \u001B[38;5;66;03m# scan the whole archive.\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/tarfile.py:2827\u001B[39m, in \u001B[36mTarFile._load\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2824\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Read through the entire archive file and look for readable\u001B[39;00m\n\u001B[32m   2825\u001B[39m \u001B[33;03m   members.\u001B[39;00m\n\u001B[32m   2826\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2827\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnext\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m   2828\u001B[39m     \u001B[38;5;28;01mpass\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/tarfile.py:2733\u001B[39m, in \u001B[36mTarFile.next\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m   2732\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m-> \u001B[39m\u001B[32m2733\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mseek\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[43m \u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[32m   2734\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m.fileobj.read(\u001B[32m1\u001B[39m):\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/gzip.py:421\u001B[39m, in \u001B[36mGzipFile.seek\u001B[39m\u001B[34m(self, offset, whence)\u001B[39m\n\u001B[32m    420\u001B[39m     \u001B[38;5;28mself\u001B[39m._check_not_closed()\n\u001B[32m--> \u001B[39m\u001B[32m421\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_buffer\u001B[49m\u001B[43m.\u001B[49m\u001B[43mseek\u001B[49m\u001B[43m(\u001B[49m\u001B[43moffset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mwhence\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    423\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.offset\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/_compression.py:153\u001B[39m, in \u001B[36mDecompressReader.seek\u001B[39m\u001B[34m(self, offset, whence)\u001B[39m\n\u001B[32m    152\u001B[39m \u001B[38;5;28;01mwhile\u001B[39;00m offset > \u001B[32m0\u001B[39m:\n\u001B[32m--> \u001B[39m\u001B[32m153\u001B[39m     data = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mmin\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mio\u001B[49m\u001B[43m.\u001B[49m\u001B[43mDEFAULT_BUFFER_SIZE\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moffset\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    154\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m data:\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/gzip.py:534\u001B[39m, in \u001B[36m_GzipReader.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m    533\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._decompressor.needs_input:\n\u001B[32m--> \u001B[39m\u001B[32m534\u001B[39m     buf = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_fp\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43mREAD_BUFFER_SIZE\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    535\u001B[39m     uncompress = \u001B[38;5;28mself\u001B[39m._decompressor.decompress(buf, size)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/gzip.py:91\u001B[39m, in \u001B[36m_PaddedFile.read\u001B[39m\u001B[34m(self, size)\u001B[39m\n\u001B[32m     90\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._read \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[32m---> \u001B[39m\u001B[32m91\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfile\u001B[49m\u001B[43m.\u001B[49m\u001B[43mread\u001B[49m\u001B[43m(\u001B[49m\u001B[43msize\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     92\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m._read + size <= \u001B[38;5;28mself\u001B[39m._length:\n",
      "\u001B[31mOSError\u001B[39m: [Errno 5] Input/output error",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[31mConnectionAbortedError\u001B[39m                    Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 53\u001B[39m\n\u001B[32m     51\u001B[39m members = []\n\u001B[32m     52\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m tp \u001B[38;5;129;01min\u001B[39;00m TAR_PATHS:\n\u001B[32m---> \u001B[39m\u001B[32m53\u001B[39m     members.extend(\u001B[43mcollect_members\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtp\u001B[49m\u001B[43m)\u001B[49m)\n\u001B[32m     55\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33mf\u001B[39m\u001B[33m\"\u001B[39m\u001B[33mCollected \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(members)\u001B[38;5;132;01m:\u001B[39;00m\u001B[33m,\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m file members from \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mlen\u001B[39m(TAR_PATHS)\u001B[38;5;132;01m}\u001B[39;00m\u001B[33m tarballs.\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m     57\u001B[39m \u001B[38;5;66;03m# 4) Pick “JSON-ish” candidates by filename; fallback to everything\u001B[39;00m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 45\u001B[39m, in \u001B[36mcollect_members\u001B[39m\u001B[34m(tar_path)\u001B[39m\n\u001B[32m     43\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mcollect_members\u001B[39m(tar_path: Path):\n\u001B[32m     44\u001B[39m     out = []\n\u001B[32m---> \u001B[39m\u001B[32m45\u001B[39m \u001B[43m    \u001B[49m\u001B[38;5;28;43;01mwith\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtarfile\u001B[49m\u001B[43m.\u001B[49m\u001B[43mopen\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtar_path\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmode\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mr:*\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mas\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtf\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     46\u001B[39m \u001B[43m        \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mtf\u001B[49m\u001B[43m.\u001B[49m\u001B[43mgetmembers\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n\u001B[32m     47\u001B[39m \u001B[43m            \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mm\u001B[49m\u001B[43m.\u001B[49m\u001B[43misfile\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m:\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/tarfile.py:2907\u001B[39m, in \u001B[36mTarFile.__exit__\u001B[39m\u001B[34m(self, type, value, traceback)\u001B[39m\n\u001B[32m   2903\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m   2904\u001B[39m     \u001B[38;5;66;03m# An exception occurred. We must not call close() because\u001B[39;00m\n\u001B[32m   2905\u001B[39m     \u001B[38;5;66;03m# it would try to write end-of-archive blocks and padding.\u001B[39;00m\n\u001B[32m   2906\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m._extfileobj:\n\u001B[32m-> \u001B[39m\u001B[32m2907\u001B[39m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   2908\u001B[39m     \u001B[38;5;28mself\u001B[39m.closed = \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/usr/lib/python3.12/gzip.py:368\u001B[39m, in \u001B[36mGzipFile.close\u001B[39m\u001B[34m(self)\u001B[39m\n\u001B[32m    366\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m myfileobj:\n\u001B[32m    367\u001B[39m     \u001B[38;5;28mself\u001B[39m.myfileobj = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m--> \u001B[39m\u001B[32m368\u001B[39m     \u001B[43mmyfileobj\u001B[49m\u001B[43m.\u001B[49m\u001B[43mclose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[31mConnectionAbortedError\u001B[39m: [Errno 103] Software caused connection abort"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "markdown",
   "id": "d2377b8a",
   "metadata": {},
   "source": [
    "## Sampling JSON objects and inferring schema\n",
    "\n",
    "TGDataset JSON files are not guaranteed to be line-delimited JSON (JSONL).\n",
    "Some files may be:\n",
    "- JSON arrays\n",
    "- Pretty-printed multi-line JSON objects\n",
    "- Gzipped JSON inside tar archives\n",
    "\n",
    "To handle this safely, we:\n",
    "- Sample a small number of objects per file\n",
    "- Detect JSONL vs JSON-array vs single-object formats\n",
    "- Parse only a bounded amount of data per file\n",
    "\n",
    "From these samples we infer:\n",
    "- Common top-level keys\n",
    "- Likely identifier fields (IDs, user IDs, message IDs)\n",
    "- Likely text or content fields\n",
    "\n",
    "This gives a **schema overview** without loading the full dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a8d465",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tarfile\n",
    "import gzip\n",
    "import io\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, Any\n",
    "\n",
    "def iter_json_objects_from_tar_member(\n",
    "    tar_path: Path,\n",
    "    member_name: str,\n",
    "    max_objects: int = 200,\n",
    "    max_json_bytes: int = 5_000_000,  # cap for array/object sampling\n",
    ") -> Iterator[Dict[str, Any]]:\n",
    "    with tarfile.open(tar_path, mode=\"r:gz\") as tf:\n",
    "        m = tf.getmember(member_name)\n",
    "        f = tf.extractfile(m)\n",
    "        if f is None:\n",
    "            return\n",
    "\n",
    "        lower = member_name.lower()\n",
    "        is_nested_gz = lower.endswith(\".gz\")\n",
    "\n",
    "        # Build a binary stream (possibly gz-decompressed)\n",
    "        raw = gzip.GzipFile(fileobj=f) if is_nested_gz else f\n",
    "\n",
    "        # Peek a prefix\n",
    "        prefix = raw.read(2048)\n",
    "        if not prefix:\n",
    "            return\n",
    "\n",
    "        # Re-open to restart stream (tar streams are not reliably seekable)\n",
    "        f2 = tf.extractfile(m)\n",
    "        if f2 is None:\n",
    "            return\n",
    "        raw2 = gzip.GzipFile(fileobj=f2) if is_nested_gz else f2\n",
    "\n",
    "        s = prefix.lstrip()\n",
    "        if s.startswith(b\"[\") or s.startswith(b\"{\"):\n",
    "            buf = raw2.read(max_json_bytes)\n",
    "            try:\n",
    "                data = json.loads(buf.decode(\"utf-8\", errors=\"replace\"))\n",
    "            except Exception:\n",
    "                return\n",
    "\n",
    "            if isinstance(data, dict):\n",
    "                yield data\n",
    "            elif isinstance(data, list):\n",
    "                for x in data:\n",
    "                    if isinstance(x, dict):\n",
    "                        yield x\n",
    "                        max_objects -= 1\n",
    "                        if max_objects <= 0:\n",
    "                            break\n",
    "            return\n",
    "\n",
    "        # Otherwise: JSONL/NDJSON\n",
    "        txt = io.TextIOWrapper(raw2, encoding=\"utf-8\", errors=\"replace\")\n",
    "        for line in txt:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if isinstance(obj, dict):\n",
    "                yield obj\n",
    "                max_objects -= 1\n",
    "                if max_objects <= 0:\n",
    "                    break\n",
    "\n",
    "# Demo: sample a few objects from the first JSON-like member\n",
    "demo = []\n",
    "for obj in iter_json_objects_from_tar_member(sample_ref[\"tar\"], sample_ref[\"name\"], max_objects=3):\n",
    "    demo.append(obj)\n",
    "demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b520bae",
   "metadata": {},
   "source": [
    "## Key/field discovery across all tar parts (sample-based)\n",
    "\n",
    "We sample up to a few hundred objects across multiple JSON-like members to infer:\n",
    "- most common top-level keys\n",
    "- likely identifier fields\n",
    "- likely text fields\n",
    "\n",
    "If this still reports `Sampled objects: 0`, it means the JSON format differs (e.g., not JSON at all, or needs a different decoder). In that case, use the earlier sniff/peek sections to identify the true format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a7a4aa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "likely_id_fields = {\"id\", \"uid\", \"user_id\", \"did\", \"rid\", \"post_id\", \"message_id\", \"cid\", \"tid\", \"chat_id\"}\n",
    "likely_text_fields = {\"text\", \"title\", \"description\", \"body\", \"content\", \"about\"}\n",
    "\n",
    "key_counts = Counter()\n",
    "id_field_counts = Counter()\n",
    "text_field_counts = Counter()\n",
    "\n",
    "sampled = 0\n",
    "MAX_OBJECTS = 500\n",
    "\n",
    "# Iterate over a subset of members across all tar parts\n",
    "for ref in sorted(json_members, key=lambda d: (d[\"tar\"].name, d[\"name\"]))[:50]:\n",
    "    for obj in iter_json_objects_from_tar_member(ref[\"tar\"], ref[\"name\"], max_objects=100):\n",
    "        sampled += 1\n",
    "        key_counts.update(obj.keys())\n",
    "\n",
    "        for f in likely_id_fields:\n",
    "            if f in obj and obj.get(f) not in (None, \"\"):\n",
    "                id_field_counts[f] += 1\n",
    "        for f in likely_text_fields:\n",
    "            if f in obj and obj.get(f) not in (None, \"\"):\n",
    "                text_field_counts[f] += 1\n",
    "\n",
    "        if sampled >= MAX_OBJECTS:\n",
    "            break\n",
    "    if sampled >= MAX_OBJECTS:\n",
    "        break\n",
    "\n",
    "print(\"Sampled objects:\", sampled)\n",
    "\n",
    "print(\"\\nMost common top-level keys:\")\n",
    "for k, c in key_counts.most_common(25):\n",
    "    print(f\"  {k:30s} {c}\")\n",
    "\n",
    "print(\"\\nLikely identifier fields (presence counts):\")\n",
    "for k, c in id_field_counts.most_common():\n",
    "    print(f\"  {k:20s} {c}\")\n",
    "\n",
    "print(\"\\nLikely title/description fields (presence counts):\")\n",
    "for k, c in text_field_counts.most_common():\n",
    "    print(f\"  {k:20s} {c}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
