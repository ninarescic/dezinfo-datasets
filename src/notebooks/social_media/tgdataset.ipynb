{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070ef7e0",
   "metadata": {},
   "source": [
    "# TGDataset (Zenodo 7640712)\n",
    "\n",
    "This notebook provides a **streaming-friendly** overview of the TGDataset Telegram channels dataset (Zenodo record `7640712`).\n",
    "\n",
    "Assumptions:\n",
    "- The dataset is already **mounted** on the server.\n",
    "- Your project has a `src/` folder and `src.core.config` exposes `SETTINGS.DATA_ROOT` (same pattern as your Higgs notebook).\n",
    "\n",
    "What you'll get:\n",
    "- file discovery + disk footprint\n",
    "- format sniffing (JSONL vs JSON array)\n",
    "- safe sampling and optional full scan for counts/time ranges\n",
    "- a quick schema/key summary to help downstream parsing\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1daa4379",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T12:24:33.717331Z",
     "start_time": "2026-01-30T12:24:33.585743Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# 0) Project setup (same pattern as Higgs notebook)\n",
    "# ============================================================\n",
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "def find_repo_root(start: Path | None = None) -> Path:\n",
    "    start = (start or Path.cwd()).resolve()\n",
    "    for p in [start, *start.parents]:\n",
    "        if (p / \"src\").is_dir():\n",
    "            return p\n",
    "    raise FileNotFoundError(\"Could not find repo root containing `src/`\")\n",
    "\n",
    "REPO_ROOT = find_repo_root()\n",
    "if str(REPO_ROOT) not in sys.path:\n",
    "    sys.path.insert(0, str(REPO_ROOT))\n",
    "\n",
    "from src.core.config import SETTINGS  # noqa: E402\n",
    "\n",
    "print(\"REPO_ROOT:\", REPO_ROOT)\n",
    "print(\"DATA_ROOT:\", SETTINGS.DATA_ROOT)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPO_ROOT: /mnt/c/Users/rescic/PycharmProjects/dezinfo-datasets\n",
      "DATA_ROOT: /home/rescic/dezinfo_data\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "ad946ffb",
   "metadata": {},
   "source": [
    "## 1) Point to the mounted dataset archive\n",
    "\n",
    "No downloads here. You’ve mounted TGDataset on your server, and the dataset is stored as a **`.tar.gz` archive**.\n",
    "\n",
    "In this section we:\n",
    "1. point to the mounted folder under `SETTINGS.DATA_ROOT`, and\n",
    "2. locate the `.tar.gz` file **without extracting it**.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "fd315084",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2026-01-30T12:25:00.212076Z",
     "start_time": "2026-01-30T12:25:00.171215Z"
    }
   },
   "source": [
    "# ============================================================\n",
    "# 1) Mounted dataset path (NO downloading / NO extracting)\n",
    "# ============================================================\n",
    "DATASET_SUBDIR = \"tgdataset\"   # <-- mount folder name\n",
    "DATASET_DIR = Path(SETTINGS.DATA_ROOT) / DATASET_SUBDIR\n",
    "assert DATASET_DIR.exists(), f\"Mounted dataset folder not found: {DATASET_DIR}\"\n",
    "print(\"DATASET_DIR:\", DATASET_DIR)\n",
    "\n",
    "# Find the .tar.gz archive in the mounted folder\n",
    "ARCHIVES = sorted(DATASET_DIR.glob(\"*.tar.gz\"))\n",
    "assert ARCHIVES, f\"No .tar.gz files found in {DATASET_DIR}\"\n",
    "TAR_PATH = ARCHIVES[0]  # if you have multiple archives, pick the right one here\n",
    "print(\"TAR_PATH:\", TAR_PATH)\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATASET_DIR: /home/rescic/dezinfo_data/tgdataset\n",
      "TAR_PATH: /home/rescic/dezinfo_data/tgdataset/TGDataset_1.tar.gz\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "c95fed28",
   "metadata": {},
   "source": [
    "## 2) Discover files inside the `.tar.gz` and disk footprint\n",
    "\n",
    "Because the dataset is archived, we can’t use `DATASET_DIR.rglob(...)`.\n",
    "\n",
    "Instead, we list members **inside** the tarball and:\n",
    "- filter for JSON-like payload files (`.json`, `.jsonl`, `.ndjson`)\n",
    "- compute a size breakdown by top-level folder *inside the archive*\n",
    "\n",
    "This gives you a quick sense of scale without extracting anything.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "f15c3ec8",
   "metadata": {},
   "source": [
    "from collections import defaultdict\n",
    "from pathlib import Path\n",
    "import tarfile\n",
    "\n",
    "def sizeof_fmt(num: int, suffix=\"B\") -> str:\n",
    "    for unit in [\"\", \"Ki\", \"Mi\", \"Gi\", \"Ti\", \"Pi\"]:\n",
    "        if abs(num) < 1024.0:\n",
    "            return f\"{num:3.1f} {unit}{suffix}\"\n",
    "        num /= 1024.0\n",
    "    return f\"{num:.1f} Ei{suffix}\"\n",
    "\n",
    "json_exts = {\".json\", \".jsonl\", \".ndjson\"}\n",
    "\n",
    "json_members = []\n",
    "folder_bytes = defaultdict(int)\n",
    "total_bytes = 0\n",
    "\n",
    "with tarfile.open(TAR_PATH, mode=\"r:gz\") as tf:\n",
    "    for m in tf.getmembers():\n",
    "        if not m.isfile():\n",
    "            continue\n",
    "        suffix = Path(m.name).suffix.lower()\n",
    "        if suffix in json_exts:\n",
    "            json_members.append(m)\n",
    "            sz = m.size\n",
    "            total_bytes += sz\n",
    "\n",
    "            parts = Path(m.name).parts\n",
    "            top = parts[0] if parts else \".\"\n",
    "            folder_bytes[top] += sz\n",
    "\n",
    "print(\"JSON-like files found (inside tar):\", len(json_members))\n",
    "assert json_members, \"No JSON-like files were found inside the .tar.gz. Check your archive / mount path.\"\n",
    "\n",
    "print(\"Total JSON footprint (inside tar):\", sizeof_fmt(total_bytes))\n",
    "\n",
    "print(\"\\nTop-level folder breakdown (inside tar):\")\n",
    "for k, v in sorted(folder_bytes.items(), key=lambda kv: kv[1], reverse=True):\n",
    "    print(f\"  - {k:20s} {sizeof_fmt(v)}\")\n",
    "\n",
    "print(\"\\nExample files (inside tar):\")\n",
    "for m in sorted(json_members, key=lambda x: x.name)[:12]:\n",
    "    print(\" \", m.name)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b58de250",
   "metadata": {},
   "source": [
    "## 3) Peek at file format (JSON array vs JSONL) — inside the archive\n",
    "\n",
    "TGDataset JSON payloads may be stored either as:\n",
    "- **JSONL/NDJSON** (one object per line), or\n",
    "- a **single JSON array** (file begins with `[`).\n",
    "\n",
    "Here we “sniff” the first few KB of a **file inside the tarball** to guess its format.\n",
    "This is a lightweight check and does not parse the full file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "791eea5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import tarfile\n",
    "import io\n",
    "\n",
    "def sniff_json_member(tar_path: Path, member_name: str, max_bytes: int = 4096) -> dict:\n",
    "    with tarfile.open(tar_path, mode=\"r:gz\") as tf:\n",
    "        m = tf.getmember(member_name)\n",
    "        f = tf.extractfile(m)\n",
    "        assert f is not None, f\"Could not extract member: {member_name}\"\n",
    "        raw = f.read(max_bytes)\n",
    "\n",
    "    ws = b\" \\t\\r\\n\"\n",
    "    i = 0\n",
    "    while i < len(raw) and raw[i:i+1] in ws:\n",
    "        i += 1\n",
    "\n",
    "    first = raw[i:i+1].decode(\"utf-8\", errors=\"replace\") if i < len(raw) else \"\"\n",
    "    text = raw.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    looks_like_array = first == \"[\"\n",
    "    looks_like_object = first == \"{\"\n",
    "    looks_like_jsonl = (\"\\n\" in text and (\"{\" in text)) and not looks_like_array\n",
    "\n",
    "    return {\n",
    "        \"member\": member_name,\n",
    "        \"first_non_ws_char\": first,\n",
    "        \"looks_like_array\": looks_like_array,\n",
    "        \"looks_like_object\": looks_like_object,\n",
    "        \"looks_like_jsonl\": looks_like_jsonl,\n",
    "    }\n",
    "\n",
    "# Pick a representative file to sniff\n",
    "sample_member = sorted(json_members, key=lambda x: x.name)[0].name\n",
    "sniff = sniff_json_member(TAR_PATH, sample_member)\n",
    "sniff\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e59e270",
   "metadata": {},
   "source": [
    "## 4) Streaming iteration over channel objects (from inside `.tar.gz`)\n",
    "\n",
    "Goal: iterate channel objects without extracting the archive or loading huge files into memory.\n",
    "\n",
    "We support two common encodings:\n",
    "- **JSONL**: stream line-by-line with `json.loads`\n",
    "- **JSON array**: stream items with `ijson` (recommended)\n",
    "\n",
    "Everything below reads file members directly from the tar archive via `tarfile.extractfile(...)`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfe3d086",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import tarfile\n",
    "import io\n",
    "from typing import Dict, Iterator, Any, Optional\n",
    "\n",
    "def iter_jsonl_objects_from_tar(tar_path: Path, member_name: str) -> Iterator[Dict[str, Any]]:\n",
    "    with tarfile.open(tar_path, mode=\"r:gz\") as tf:\n",
    "        m = tf.getmember(member_name)\n",
    "        f = tf.extractfile(m)\n",
    "        assert f is not None, f\"Could not extract member: {member_name}\"\n",
    "        txt = io.TextIOWrapper(f, encoding=\"utf-8\", errors=\"replace\")\n",
    "        for line in txt:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            yield json.loads(line)\n",
    "\n",
    "def iter_json_array_objects_with_ijson_from_tar(\n",
    "    tar_path: Path,\n",
    "    member_name: str,\n",
    "    array_item_prefix: str = \"item\",\n",
    ") -> Iterator[Dict[str, Any]]:\n",
    "    try:\n",
    "        import ijson  # type: ignore\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"This file looks like a JSON array. Install ijson to stream it: `pip install ijson`\"\n",
    "        ) from e\n",
    "\n",
    "    with tarfile.open(tar_path, mode=\"r:gz\") as tf:\n",
    "        m = tf.getmember(member_name)\n",
    "        f = tf.extractfile(m)\n",
    "        assert f is not None, f\"Could not extract member: {member_name}\"\n",
    "        for obj in ijson.items(f, array_item_prefix):\n",
    "            yield obj\n",
    "\n",
    "def iter_channel_objects_from_tar(tar_path: Path, member_name: str, sniff: Optional[dict] = None):\n",
    "    sniff = sniff or sniff_json_member(tar_path, member_name)\n",
    "    if sniff.get(\"looks_like_array\"):\n",
    "        return iter_json_array_objects_with_ijson_from_tar(tar_path, member_name)\n",
    "    return iter_jsonl_objects_from_tar(tar_path, member_name)\n",
    "\n",
    "# Quick demo: iterate a few objects from the sample member\n",
    "it = iter_channel_objects_from_tar(TAR_PATH, sample_member, sniff=sniff)\n",
    "demo = []\n",
    "for _, obj in zip(range(3), it):\n",
    "    demo.append(obj)\n",
    "demo\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c2f267",
   "metadata": {},
   "source": [
    "## 5) Dataset stats (sample mode by default) — scanning members in the tarball\n",
    "\n",
    "TGDataset can be very large. A full scan may take a while.\n",
    "\n",
    "We provide two modes:\n",
    "- **Sample scan** (default): quick scan over the first `n_files` members, limited objects per member.\n",
    "- **Full scan**: set `full_scan=True` to scan all JSON-like members.\n",
    "\n",
    "We try (best-effort) to estimate:\n",
    "- number of channel objects scanned\n",
    "- number of messages scanned (if a channel has a list field like `messages`)\n",
    "- approximate min/max timestamps (if timestamps are present)\n",
    "\n",
    "Because TGDataset schema variants exist, you may need to adjust `iter_messages_from_channel()`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ff61a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Iterable, Optional, Any, Dict\n",
    "\n",
    "@dataclass\n",
    "class ScanConfig:\n",
    "    full_scan: bool = False\n",
    "    n_files: int = 5\n",
    "    n_channels_per_file: int = 200\n",
    "    n_messages_per_channel: int = 2000  # cap in sample mode\n",
    "\n",
    "CFG = ScanConfig(full_scan=False)\n",
    "\n",
    "def parse_any_datetime(x) -> Optional[datetime]:\n",
    "    if x is None:\n",
    "        return None\n",
    "    if isinstance(x, (int, float)):\n",
    "        ts = float(x)\n",
    "        if ts > 1e12:  # ms -> s\n",
    "            ts /= 1000.0\n",
    "        try:\n",
    "            return datetime.utcfromtimestamp(ts)\n",
    "        except Exception:\n",
    "            return None\n",
    "    if isinstance(x, str):\n",
    "        # try python's ISO parser first\n",
    "        try:\n",
    "            return datetime.fromisoformat(x.replace(\"Z\", \"+00:00\"))\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "def iter_messages_from_channel(channel: Dict[str, Any]) -> Iterable[Dict[str, Any]]:\n",
    "    if isinstance(channel.get(\"messages\"), list):\n",
    "        return channel[\"messages\"]\n",
    "    if isinstance(channel.get(\"channel\"), dict) and isinstance(channel[\"channel\"].get(\"messages\"), list):\n",
    "        return channel[\"channel\"][\"messages\"]\n",
    "    return []\n",
    "\n",
    "def extract_message_timestamp(msg: Dict[str, Any]) -> Optional[datetime]:\n",
    "    for k in (\"date\", \"datetime\", \"timestamp\", \"ts\", \"time\", \"created_at\"):\n",
    "        if k in msg:\n",
    "            dt = parse_any_datetime(msg.get(k))\n",
    "            if dt:\n",
    "                return dt\n",
    "    return None\n",
    "\n",
    "members_sorted = sorted(json_members, key=lambda x: x.name)\n",
    "members_to_scan = members_sorted if CFG.full_scan else members_sorted[:CFG.n_files]\n",
    "\n",
    "channels_scanned = 0\n",
    "messages_scanned = 0\n",
    "min_dt = None\n",
    "max_dt = None\n",
    "\n",
    "for m in members_to_scan:\n",
    "    member_name = m.name\n",
    "    sniff_m = sniff_json_member(TAR_PATH, member_name)\n",
    "    it = iter_channel_objects_from_tar(TAR_PATH, member_name, sniff=sniff_m)\n",
    "\n",
    "    for i, channel in enumerate(it):\n",
    "        if not CFG.full_scan and i >= CFG.n_channels_per_file:\n",
    "            break\n",
    "\n",
    "        channels_scanned += 1\n",
    "\n",
    "        msgs = list(iter_messages_from_channel(channel))\n",
    "        if not CFG.full_scan:\n",
    "            msgs = msgs[:CFG.n_messages_per_channel]\n",
    "\n",
    "        messages_scanned += len(msgs)\n",
    "\n",
    "        for msg in msgs:\n",
    "            if not isinstance(msg, dict):\n",
    "                continue\n",
    "            dt = extract_message_timestamp(msg)\n",
    "            if not dt:\n",
    "                continue\n",
    "            min_dt = dt if (min_dt is None or dt < min_dt) else min_dt\n",
    "            max_dt = dt if (max_dt is None or dt > max_dt) else max_dt\n",
    "\n",
    "print(\"Members scanned:\", len(members_to_scan))\n",
    "print(\"Channels scanned:\", channels_scanned)\n",
    "print(\"Messages scanned (best-effort):\", messages_scanned)\n",
    "print(\"Min timestamp (best-effort):\", min_dt)\n",
    "print(\"Max timestamp (best-effort):\", max_dt)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c158344",
   "metadata": {},
   "source": [
    "## 6) Schema/key summary from sampled channels (tar-aware)\n",
    "\n",
    "Different TGDataset dumps can vary slightly in which top-level fields are present per channel object.\n",
    "\n",
    "We sample channel objects across a few archive members and summarize:\n",
    "- most common top-level keys\n",
    "- a few likely identifiers (username/id/title)\n",
    "\n",
    "This helps you adapt parsing and feature extraction quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cb6069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from itertools import islice\n",
    "\n",
    "def sample_channels_from_tar(members, n_total: int = 200, per_member: int = 50) -> list[dict]:\n",
    "    out = []\n",
    "    for m in members:\n",
    "        member_name = m.name if hasattr(m, \"name\") else str(m)\n",
    "        try:\n",
    "            sniff_m = sniff_json_member(TAR_PATH, member_name)\n",
    "            it = iter_channel_objects_from_tar(TAR_PATH, member_name, sniff=sniff_m)\n",
    "            for ch in islice(it, per_member):\n",
    "                out.append(ch)\n",
    "                if len(out) >= n_total:\n",
    "                    return out\n",
    "        except Exception:\n",
    "            continue\n",
    "    return out\n",
    "\n",
    "sample = sample_channels_from_tar(members_to_scan, n_total=200, per_member=50)\n",
    "print(\"Sampled channels:\", len(sample))\n",
    "\n",
    "key_counts = Counter()\n",
    "id_like = Counter()\n",
    "title_like = Counter()\n",
    "\n",
    "for ch in sample:\n",
    "    if not isinstance(ch, dict):\n",
    "        continue\n",
    "    key_counts.update(ch.keys())\n",
    "\n",
    "    for k in (\"id\", \"channel_id\", \"chat_id\", \"username\", \"user\", \"name\"):\n",
    "        if k in ch and ch.get(k) not in (None, \"\"):\n",
    "            id_like[k] += 1\n",
    "    for k in (\"title\", \"channel_title\", \"display_name\", \"about\", \"description\"):\n",
    "        if k in ch and ch.get(k) not in (None, \"\"):\n",
    "            title_like[k] += 1\n",
    "\n",
    "print(\"\\nMost common top-level keys:\")\n",
    "for k, v in key_counts.most_common(25):\n",
    "    print(f\"  - {k:25s} {v}\")\n",
    "\n",
    "print(\"\\nLikely identifier fields (presence counts):\")\n",
    "for k, v in id_like.most_common():\n",
    "    print(f\"  - {k:25s} {v}\")\n",
    "\n",
    "print(\"\\nLikely title/description fields (presence counts):\")\n",
    "for k, v in title_like.most_common():\n",
    "    print(f\"  - {k:25s} {v}\")\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3355ceb4f6c20fa9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "name": "python3",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
